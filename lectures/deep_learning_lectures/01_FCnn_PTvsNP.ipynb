{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Computations involved in deep learning are mainly about linear algebra. NumPy is very optimized for operations on multidimensional arrays. \n",
    "\n",
    "**Why should we spend time learning a deep learning framework?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fully connected NN with NumPy\n",
    "https://chih-ling-hsu.github.io/2017/08/30/NN-XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "     \n",
    "def tanh_derivative(x):\n",
    "    return 1 / np.cosh(x) ** 2\n",
    "\n",
    "\n",
    "class FullyConnectedNN_np:\n",
    "    #########\n",
    "    # parameters\n",
    "    # ----------\n",
    "    # net_arch:  consists of a list of integers, indicating\n",
    "    #            the number of neurons in each layer, i.e. the network architecture\n",
    "    #########\n",
    "    def __init__(self, net_arch):        \n",
    "        self.activity = np.tanh\n",
    "        self.activity_derivative = tanh_derivative\n",
    "        self.layers = len(net_arch)\n",
    "        self.steps_per_epoch = 1\n",
    "        self.arch = net_arch\n",
    "        self.weights = []\n",
    "\n",
    "        # Random initialization with range of weight values (-1,1)\n",
    "        for layer in range(self.layers - 1):\n",
    "            w = 2*np.random.rand(net_arch[layer] + 1, net_arch[layer+1]) - 1\n",
    "            self.weights.append(w)\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        y = x\n",
    "\n",
    "        for i in range(len(self.weights)-1):\n",
    "            activation = np.dot(y[i], self.weights[i])\n",
    "            activity = self.activity(activation)\n",
    "\n",
    "            # add the bias for the next layer\n",
    "            activity = np.concatenate((np.ones(1), np.array(activity)))\n",
    "            y.append(activity)\n",
    "\n",
    "        # last layer\n",
    "        activation = np.dot(y[-1], self.weights[-1])\n",
    "        activity = self.activity(activation)\n",
    "        y.append(activity)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def _back_prop(self, y, target, learning_rate):\n",
    "        error = target - y[-1]\n",
    "        delta_vec = [error * self.activity_derivative(y[-1])]\n",
    "\n",
    "        # we need to begin from the back, from the next to last layer\n",
    "        for i in range(self.layers-2, 0, -1):\n",
    "            error = delta_vec[-1].dot(self.weights[i][1:].T)\n",
    "            error = error*self.activity_derivative(y[i][1:])\n",
    "            delta_vec.append(error)\n",
    "\n",
    "        # Now we need to set the values from back to front\n",
    "        delta_vec.reverse()\n",
    "        \n",
    "        # Finally, we adjust the weights, using the backpropagation rules\n",
    "        for i in range(len(self.weights)):\n",
    "            layer = y[i].reshape(1, self.arch[i]+1)\n",
    "            delta = delta_vec[i].reshape(1, self.arch[i+1])\n",
    "            self.weights[i] += learning_rate*layer.T.dot(delta)\n",
    "    \n",
    "    #########\n",
    "    # parameters\n",
    "    # ----------\n",
    "    # data:    the set of all possible pairs of booleans True or False indicated by the integers 1 or 0\n",
    "    # labels:  the result of the logical operation 'xor' on each of those input pairs\n",
    "    #########\n",
    "    def fit(self, data, labels, learning_rate=0.1, epochs=100):\n",
    "        \n",
    "        # Add bias units to the input layer - \n",
    "        # add a \"1\" to the input data (the always-on bias neuron)\n",
    "        ones = np.ones((1, data.shape[0]))\n",
    "        Z = np.concatenate((ones.T, data), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            if (k+1) % 10000 == 0:\n",
    "                print('epochs: {}'.format(k+1))\n",
    "        \n",
    "            sample = np.random.randint(X.shape[0])\n",
    "\n",
    "            # We will now go ahead and set up our feed-forward propagation:\n",
    "            x = [Z[sample]]\n",
    "            y = self._forward_prop(x)\n",
    "\n",
    "            # Now we do our back-propagation of the error to adjust the weights:\n",
    "            target = labels[sample]\n",
    "            self._back_prop(y, target, learning_rate)\n",
    "    \n",
    "    #########\n",
    "    # the predict function is used to check the prediction result of\n",
    "    # this neural network.\n",
    "    # \n",
    "    # parameters\n",
    "    # ----------\n",
    "    # x:      single input data\n",
    "    #########\n",
    "    def predict_single_data(self, x):\n",
    "        val = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "        for i in range(0, len(self.weights)):\n",
    "            val = self.activity(np.dot(val, self.weights[i]))\n",
    "            val = np.concatenate((np.ones(1).T, np.array(val)))\n",
    "        return val[1]\n",
    "    \n",
    "    #########\n",
    "    # the predict function is used to check the prediction result of\n",
    "    # this neural network.\n",
    "    # \n",
    "    # parameters\n",
    "    # ----------\n",
    "    # X:      the input data array\n",
    "    #########\n",
    "    def predict(self, X):\n",
    "        Y = np.array([]).reshape(0, self.arch[-1])\n",
    "        for x in X:\n",
    "            y = np.array([[self.predict_single_data(x)]])\n",
    "            Y = np.vstack((Y,y))\n",
    "        return Y\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input data\n",
    "X = np.array([[0, 0], [0, 1],\n",
    "                [1, 0], [1, 1]])\n",
    "\n",
    "# Set the labels, the correct results for the xor operation\n",
    "y = np.array([0, 1, \n",
    "                 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction\n",
      "[0 0] 0.0035321878676498617\n",
      "[0 1] 0.9799502402444462\n",
      "[1 0] 0.9784029892880758\n",
      "[1 1] 0.020612293862843356\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(41)\n",
    "\n",
    "net = FullyConnectedNN_np([2, 2, 1])\n",
    "\n",
    "net.fit(X, y, epochs=4000)\n",
    "\n",
    "print(\"Final prediction\")\n",
    "for s in X:\n",
    "    print(s, net.predict_single_data(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fully connected NN with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Set the input data and responses\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnectedNN_pt(net_arch):\n",
    "    layers = []\n",
    "    for i in range(len(net_arch) - 2):\n",
    "        layers.append(nn.Linear(net_arch[i], net_arch[i + 1]))\n",
    "        layers.append(nn.Tanh())\n",
    "    layers.append(nn.Linear(net_arch[-2], net_arch[-1]))\n",
    "    layers.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction\n",
      "[0. 0.] [0.01678943]\n",
      "[0. 1.] [0.99052423]\n",
      "[1. 0.] [0.9905206]\n",
      "[1. 1.] [0.01770484]\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "torch.manual_seed(0)\n",
    "\n",
    "net = FullyConnectedNN_pt([2, 2, 1])\n",
    "\n",
    "lr = 0.1\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "for e in range(4000):\n",
    "    y_pred = net(X)    \n",
    "    loss = criterion(y_pred, y)\n",
    "    #display.clear_output(wait=True)\n",
    "    #print(\"EPOCH: {} | BCE: {}\".format(e, loss.item()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"Final prediction\")\n",
    "for s in X:\n",
    "    print(s.numpy(), net(s).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "- Deep learning models are quite standardized in many of their aspects\n",
    "- PyTorch provides a flexible and optimized way to automatically compute gradients\n",
    "- There is a large, active community behind it\n",
    "- Some nice people wrote a lot of flexible, well-optimized code for us. And I assume a significant portion of us is not made of elves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "https://github.com/pytorch/examples\n",
    "\n",
    "https://pytorch.org/tutorials/\n",
    "\n",
    "https://pytorch.org/resources/\n",
    "\n",
    "https://pytorch.org/docs/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
